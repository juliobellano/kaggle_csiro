{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:58:02.732570Z","iopub.execute_input":"2025-12-21T16:58:02.733305Z","iopub.status.idle":"2025-12-21T16:58:02.889853Z","shell.execute_reply.started":"2025-12-21T16:58:02.733250Z","shell.execute_reply":"2025-12-21T16:58:02.889269Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"/kaggle/input/csiro-biomass/sample_submission.csv\n/kaggle/input/csiro-biomass/train.csv\n/kaggle/input/csiro-biomass/test.csv\n/kaggle/input/csiro-biomass/test/ID1001187975.jpg\n/kaggle/input/csiro-biomass/train/ID2099464826.jpg\n/kaggle/input/csiro-biomass/train/ID2037861084.jpg\n/kaggle/input/csiro-biomass/train/ID1211362607.jpg\n/kaggle/input/csiro-biomass/train/ID1853508321.jpg\n/kaggle/input/csiro-biomass/train/ID193102215.jpg\n/kaggle/input/csiro-biomass/train/ID698608346.jpg\n/kaggle/input/csiro-biomass/train/ID1859251563.jpg\n/kaggle/input/csiro-biomass/train/ID1880764911.jpg\n/kaggle/input/csiro-biomass/train/ID853954911.jpg\n/kaggle/input/csiro-biomass/train/ID1403107574.jpg\n/kaggle/input/csiro-biomass/train/ID1781353117.jpg\n/kaggle/input/csiro-biomass/train/ID384648061.jpg\n/kaggle/input/csiro-biomass/train/ID1563418511.jpg\n/kaggle/input/csiro-biomass/train/ID2125100696.jpg\n/kaggle/input/csiro-biomass/train/ID482555369.jpg\n/kaggle/input/csiro-biomass/train/ID638711343.jpg\n/kaggle/input/csiro-biomass/train/ID779628955.jpg\n/kaggle/input/csiro-biomass/train/ID1876271942.jpg\n/kaggle/input/csiro-biomass/train/ID1692894460.jpg\n/kaggle/input/csiro-biomass/train/ID746335827.jpg\n/kaggle/input/csiro-biomass/train/ID1136169672.jpg\n/kaggle/input/csiro-biomass/train/ID1471216911.jpg\n/kaggle/input/csiro-biomass/train/ID846154859.jpg\n/kaggle/input/csiro-biomass/train/ID1294770420.jpg\n/kaggle/input/csiro-biomass/train/ID1183807388.jpg\n/kaggle/input/csiro-biomass/train/ID423506847.jpg\n/kaggle/input/csiro-biomass/train/ID1889150649.jpg\n/kaggle/input/csiro-biomass/train/ID1140993511.jpg\n/kaggle/input/csiro-biomass/train/ID1413758094.jpg\n/kaggle/input/csiro-biomass/train/ID1545077474.jpg\n/kaggle/input/csiro-biomass/train/ID95050718.jpg\n/kaggle/input/csiro-biomass/train/ID528010569.jpg\n/kaggle/input/csiro-biomass/train/ID1645161155.jpg\n/kaggle/input/csiro-biomass/train/ID786365141.jpg\n/kaggle/input/csiro-biomass/train/ID896386823.jpg\n/kaggle/input/csiro-biomass/train/ID1025234388.jpg\n/kaggle/input/csiro-biomass/train/ID663006174.jpg\n/kaggle/input/csiro-biomass/train/ID1509266870.jpg\n/kaggle/input/csiro-biomass/train/ID1496750796.jpg\n/kaggle/input/csiro-biomass/train/ID471758347.jpg\n/kaggle/input/csiro-biomass/train/ID740402124.jpg\n/kaggle/input/csiro-biomass/train/ID1624268863.jpg\n/kaggle/input/csiro-biomass/train/ID1098771283.jpg\n/kaggle/input/csiro-biomass/train/ID710341728.jpg\n/kaggle/input/csiro-biomass/train/ID2086966681.jpg\n/kaggle/input/csiro-biomass/train/ID1573329652.jpg\n/kaggle/input/csiro-biomass/train/ID54128926.jpg\n/kaggle/input/csiro-biomass/train/ID50027657.jpg\n/kaggle/input/csiro-biomass/train/ID1559189397.jpg\n/kaggle/input/csiro-biomass/train/ID290369222.jpg\n/kaggle/input/csiro-biomass/train/ID1590632667.jpg\n/kaggle/input/csiro-biomass/train/ID552040066.jpg\n/kaggle/input/csiro-biomass/train/ID488873801.jpg\n/kaggle/input/csiro-biomass/train/ID363069566.jpg\n/kaggle/input/csiro-biomass/train/ID1839139621.jpg\n/kaggle/input/csiro-biomass/train/ID1131079710.jpg\n/kaggle/input/csiro-biomass/train/ID2010625680.jpg\n/kaggle/input/csiro-biomass/train/ID152157478.jpg\n/kaggle/input/csiro-biomass/train/ID1357758282.jpg\n/kaggle/input/csiro-biomass/train/ID1498398599.jpg\n/kaggle/input/csiro-biomass/train/ID679913293.jpg\n/kaggle/input/csiro-biomass/train/ID697718693.jpg\n/kaggle/input/csiro-biomass/train/ID4464212.jpg\n/kaggle/input/csiro-biomass/train/ID1275072698.jpg\n/kaggle/input/csiro-biomass/train/ID1579942839.jpg\n/kaggle/input/csiro-biomass/train/ID799079114.jpg\n/kaggle/input/csiro-biomass/train/ID1415329644.jpg\n/kaggle/input/csiro-biomass/train/ID1510574031.jpg\n/kaggle/input/csiro-biomass/train/ID1078930021.jpg\n/kaggle/input/csiro-biomass/train/ID1456861072.jpg\n/kaggle/input/csiro-biomass/train/ID930534670.jpg\n/kaggle/input/csiro-biomass/train/ID13162390.jpg\n/kaggle/input/csiro-biomass/train/ID567744300.jpg\n/kaggle/input/csiro-biomass/train/ID344618040.jpg\n/kaggle/input/csiro-biomass/train/ID566966892.jpg\n/kaggle/input/csiro-biomass/train/ID1437386574.jpg\n/kaggle/input/csiro-biomass/train/ID667059550.jpg\n/kaggle/input/csiro-biomass/train/ID72895391.jpg\n/kaggle/input/csiro-biomass/train/ID1193692654.jpg\n/kaggle/input/csiro-biomass/train/ID1386202352.jpg\n/kaggle/input/csiro-biomass/train/ID871463897.jpg\n/kaggle/input/csiro-biomass/train/ID2096636211.jpg\n/kaggle/input/csiro-biomass/train/ID2003438517.jpg\n/kaggle/input/csiro-biomass/train/ID21377800.jpg\n/kaggle/input/csiro-biomass/train/ID230058600.jpg\n/kaggle/input/csiro-biomass/train/ID1753847361.jpg\n/kaggle/input/csiro-biomass/train/ID1512751450.jpg\n/kaggle/input/csiro-biomass/train/ID12390962.jpg\n/kaggle/input/csiro-biomass/train/ID1746343319.jpg\n/kaggle/input/csiro-biomass/train/ID978026131.jpg\n/kaggle/input/csiro-biomass/train/ID383231615.jpg\n/kaggle/input/csiro-biomass/train/ID146920896.jpg\n/kaggle/input/csiro-biomass/train/ID1036339023.jpg\n/kaggle/input/csiro-biomass/train/ID1168534540.jpg\n/kaggle/input/csiro-biomass/train/ID1859792585.jpg\n/kaggle/input/csiro-biomass/train/ID1251029854.jpg\n/kaggle/input/csiro-biomass/train/ID1113329413.jpg\n/kaggle/input/csiro-biomass/train/ID1874904894.jpg\n/kaggle/input/csiro-biomass/train/ID1671844336.jpg\n/kaggle/input/csiro-biomass/train/ID1831254380.jpg\n/kaggle/input/csiro-biomass/train/ID1103883611.jpg\n/kaggle/input/csiro-biomass/train/ID797502182.jpg\n/kaggle/input/csiro-biomass/train/ID1784585001.jpg\n/kaggle/input/csiro-biomass/train/ID1058383417.jpg\n/kaggle/input/csiro-biomass/train/ID1488408526.jpg\n/kaggle/input/csiro-biomass/train/ID429799190.jpg\n/kaggle/input/csiro-biomass/train/ID1291116815.jpg\n/kaggle/input/csiro-biomass/train/ID1516374298.jpg\n/kaggle/input/csiro-biomass/train/ID1618597318.jpg\n/kaggle/input/csiro-biomass/train/ID1345375788.jpg\n/kaggle/input/csiro-biomass/train/ID686797154.jpg\n/kaggle/input/csiro-biomass/train/ID1139866256.jpg\n/kaggle/input/csiro-biomass/train/ID1149598723.jpg\n/kaggle/input/csiro-biomass/train/ID212206250.jpg\n/kaggle/input/csiro-biomass/train/ID112966473.jpg\n/kaggle/input/csiro-biomass/train/ID1540480250.jpg\n/kaggle/input/csiro-biomass/train/ID544444725.jpg\n/kaggle/input/csiro-biomass/train/ID1513184765.jpg\n/kaggle/input/csiro-biomass/train/ID668330410.jpg\n/kaggle/input/csiro-biomass/train/ID1444674500.jpg\n/kaggle/input/csiro-biomass/train/ID1962379474.jpg\n/kaggle/input/csiro-biomass/train/ID605134229.jpg\n/kaggle/input/csiro-biomass/train/ID914754166.jpg\n/kaggle/input/csiro-biomass/train/ID354528442.jpg\n/kaggle/input/csiro-biomass/train/ID950496197.jpg\n/kaggle/input/csiro-biomass/train/ID1395011773.jpg\n/kaggle/input/csiro-biomass/train/ID1357768767.jpg\n/kaggle/input/csiro-biomass/train/ID210865340.jpg\n/kaggle/input/csiro-biomass/train/ID936984905.jpg\n/kaggle/input/csiro-biomass/train/ID1976436386.jpg\n/kaggle/input/csiro-biomass/train/ID1215977190.jpg\n/kaggle/input/csiro-biomass/train/ID803479541.jpg\n/kaggle/input/csiro-biomass/train/ID1244346858.jpg\n/kaggle/input/csiro-biomass/train/ID158170916.jpg\n/kaggle/input/csiro-biomass/train/ID1208644039.jpg\n/kaggle/input/csiro-biomass/train/ID1314135397.jpg\n/kaggle/input/csiro-biomass/train/ID1012260530.jpg\n/kaggle/input/csiro-biomass/train/ID1053972079.jpg\n/kaggle/input/csiro-biomass/train/ID656251220.jpg\n/kaggle/input/csiro-biomass/train/ID1084819986.jpg\n/kaggle/input/csiro-biomass/train/ID1337107565.jpg\n/kaggle/input/csiro-biomass/train/ID1268934251.jpg\n/kaggle/input/csiro-biomass/train/ID617132135.jpg\n/kaggle/input/csiro-biomass/train/ID1472525822.jpg\n/kaggle/input/csiro-biomass/train/ID668475812.jpg\n/kaggle/input/csiro-biomass/train/ID681680726.jpg\n/kaggle/input/csiro-biomass/train/ID1476045099.jpg\n/kaggle/input/csiro-biomass/train/ID1570190541.jpg\n/kaggle/input/csiro-biomass/train/ID1403078396.jpg\n/kaggle/input/csiro-biomass/train/ID2030696575.jpg\n/kaggle/input/csiro-biomass/train/ID1782608354.jpg\n/kaggle/input/csiro-biomass/train/ID194823383.jpg\n/kaggle/input/csiro-biomass/train/ID196516535.jpg\n/kaggle/input/csiro-biomass/train/ID212206832.jpg\n/kaggle/input/csiro-biomass/train/ID1638922597.jpg\n/kaggle/input/csiro-biomass/train/ID1457700382.jpg\n/kaggle/input/csiro-biomass/train/ID1989506559.jpg\n/kaggle/input/csiro-biomass/train/ID789169173.jpg\n/kaggle/input/csiro-biomass/train/ID1634731537.jpg\n/kaggle/input/csiro-biomass/train/ID1428837636.jpg\n/kaggle/input/csiro-biomass/train/ID2006686196.jpg\n/kaggle/input/csiro-biomass/train/ID885388135.jpg\n/kaggle/input/csiro-biomass/train/ID1789853061.jpg\n/kaggle/input/csiro-biomass/train/ID1655778545.jpg\n/kaggle/input/csiro-biomass/train/ID697059386.jpg\n/kaggle/input/csiro-biomass/train/ID121331988.jpg\n/kaggle/input/csiro-biomass/train/ID2099742797.jpg\n/kaggle/input/csiro-biomass/train/ID342818398.jpg\n/kaggle/input/csiro-biomass/train/ID317990700.jpg\n/kaggle/input/csiro-biomass/train/ID706288721.jpg\n/kaggle/input/csiro-biomass/train/ID1159071020.jpg\n/kaggle/input/csiro-biomass/train/ID755710743.jpg\n/kaggle/input/csiro-biomass/train/ID1254829053.jpg\n/kaggle/input/csiro-biomass/train/ID475010202.jpg\n/kaggle/input/csiro-biomass/train/ID1693880739.jpg\n/kaggle/input/csiro-biomass/train/ID1894998379.jpg\n/kaggle/input/csiro-biomass/train/ID48303557.jpg\n/kaggle/input/csiro-biomass/train/ID1385921939.jpg\n/kaggle/input/csiro-biomass/train/ID147528735.jpg\n/kaggle/input/csiro-biomass/train/ID407646960.jpg\n/kaggle/input/csiro-biomass/train/ID1035947949.jpg\n/kaggle/input/csiro-biomass/train/ID1119761112.jpg\n/kaggle/input/csiro-biomass/train/ID1988033238.jpg\n/kaggle/input/csiro-biomass/train/ID1857489997.jpg\n/kaggle/input/csiro-biomass/train/ID742198710.jpg\n/kaggle/input/csiro-biomass/train/ID588120964.jpg\n/kaggle/input/csiro-biomass/train/ID431471530.jpg\n/kaggle/input/csiro-biomass/train/ID353424190.jpg\n/kaggle/input/csiro-biomass/train/ID380752847.jpg\n/kaggle/input/csiro-biomass/train/ID2069766023.jpg\n/kaggle/input/csiro-biomass/train/ID600602588.jpg\n/kaggle/input/csiro-biomass/train/ID560946727.jpg\n/kaggle/input/csiro-biomass/train/ID1011485656.jpg\n/kaggle/input/csiro-biomass/train/ID808079729.jpg\n/kaggle/input/csiro-biomass/train/ID1217108125.jpg\n/kaggle/input/csiro-biomass/train/ID1623964968.jpg\n/kaggle/input/csiro-biomass/train/ID980878870.jpg\n/kaggle/input/csiro-biomass/train/ID793526563.jpg\n/kaggle/input/csiro-biomass/train/ID397994621.jpg\n/kaggle/input/csiro-biomass/train/ID975115267.jpg\n/kaggle/input/csiro-biomass/train/ID1237349078.jpg\n/kaggle/input/csiro-biomass/train/ID684383343.jpg\n/kaggle/input/csiro-biomass/train/ID866684633.jpg\n/kaggle/input/csiro-biomass/train/ID1665142816.jpg\n/kaggle/input/csiro-biomass/train/ID2048645043.jpg\n/kaggle/input/csiro-biomass/train/ID1953171547.jpg\n/kaggle/input/csiro-biomass/train/ID1451025862.jpg\n/kaggle/input/csiro-biomass/train/ID71885430.jpg\n/kaggle/input/csiro-biomass/train/ID307060225.jpg\n/kaggle/input/csiro-biomass/train/ID969218269.jpg\n/kaggle/input/csiro-biomass/train/ID980538882.jpg\n/kaggle/input/csiro-biomass/train/ID1028611175.jpg\n/kaggle/input/csiro-biomass/train/ID670276799.jpg\n/kaggle/input/csiro-biomass/train/ID2002797732.jpg\n/kaggle/input/csiro-biomass/train/ID1374789439.jpg\n/kaggle/input/csiro-biomass/train/ID473494649.jpg\n/kaggle/input/csiro-biomass/train/ID1993907137.jpg\n/kaggle/input/csiro-biomass/train/ID1962197151.jpg\n/kaggle/input/csiro-biomass/train/ID828217731.jpg\n/kaggle/input/csiro-biomass/train/ID972274220.jpg\n/kaggle/input/csiro-biomass/train/ID1954669045.jpg\n/kaggle/input/csiro-biomass/train/ID1354190372.jpg\n/kaggle/input/csiro-biomass/train/ID1458758610.jpg\n/kaggle/input/csiro-biomass/train/ID40849327.jpg\n/kaggle/input/csiro-biomass/train/ID1952813879.jpg\n/kaggle/input/csiro-biomass/train/ID572336285.jpg\n/kaggle/input/csiro-biomass/train/ID1473228876.jpg\n/kaggle/input/csiro-biomass/train/ID1963715583.jpg\n/kaggle/input/csiro-biomass/train/ID1463690813.jpg\n/kaggle/input/csiro-biomass/train/ID1899025384.jpg\n/kaggle/input/csiro-biomass/train/ID386216505.jpg\n/kaggle/input/csiro-biomass/train/ID1789265307.jpg\n/kaggle/input/csiro-biomass/train/ID315357834.jpg\n/kaggle/input/csiro-biomass/train/ID2089023774.jpg\n/kaggle/input/csiro-biomass/train/ID520514019.jpg\n/kaggle/input/csiro-biomass/train/ID1970522802.jpg\n/kaggle/input/csiro-biomass/train/ID1139918758.jpg\n/kaggle/input/csiro-biomass/train/ID1051144034.jpg\n/kaggle/input/csiro-biomass/train/ID1370004842.jpg\n/kaggle/input/csiro-biomass/train/ID761508093.jpg\n/kaggle/input/csiro-biomass/train/ID2052993274.jpg\n/kaggle/input/csiro-biomass/train/ID1277756619.jpg\n/kaggle/input/csiro-biomass/train/ID6269659.jpg\n/kaggle/input/csiro-biomass/train/ID1574125908.jpg\n/kaggle/input/csiro-biomass/train/ID135365668.jpg\n/kaggle/input/csiro-biomass/train/ID1182523622.jpg\n/kaggle/input/csiro-biomass/train/ID554314721.jpg\n/kaggle/input/csiro-biomass/train/ID1049634115.jpg\n/kaggle/input/csiro-biomass/train/ID1127246618.jpg\n/kaggle/input/csiro-biomass/train/ID900012207.jpg\n/kaggle/input/csiro-biomass/train/ID574213894.jpg\n/kaggle/input/csiro-biomass/train/ID415656958.jpg\n/kaggle/input/csiro-biomass/train/ID61833032.jpg\n/kaggle/input/csiro-biomass/train/ID2053315094.jpg\n/kaggle/input/csiro-biomass/train/ID550623196.jpg\n/kaggle/input/csiro-biomass/train/ID657448172.jpg\n/kaggle/input/csiro-biomass/train/ID1675365449.jpg\n/kaggle/input/csiro-biomass/train/ID2014192906.jpg\n/kaggle/input/csiro-biomass/train/ID162394992.jpg\n/kaggle/input/csiro-biomass/train/ID968643034.jpg\n/kaggle/input/csiro-biomass/train/ID684062938.jpg\n/kaggle/input/csiro-biomass/train/ID802547515.jpg\n/kaggle/input/csiro-biomass/train/ID294150104.jpg\n/kaggle/input/csiro-biomass/train/ID1618145129.jpg\n/kaggle/input/csiro-biomass/train/ID956512130.jpg\n/kaggle/input/csiro-biomass/train/ID142751858.jpg\n/kaggle/input/csiro-biomass/train/ID325799913.jpg\n/kaggle/input/csiro-biomass/train/ID443091455.jpg\n/kaggle/input/csiro-biomass/train/ID661372352.jpg\n/kaggle/input/csiro-biomass/train/ID1062837331.jpg\n/kaggle/input/csiro-biomass/train/ID498304885.jpg\n/kaggle/input/csiro-biomass/train/ID187238869.jpg\n/kaggle/input/csiro-biomass/train/ID1450399782.jpg\n/kaggle/input/csiro-biomass/train/ID2056023629.jpg\n/kaggle/input/csiro-biomass/train/ID576621307.jpg\n/kaggle/input/csiro-biomass/train/ID1199150612.jpg\n/kaggle/input/csiro-biomass/train/ID1411613934.jpg\n/kaggle/input/csiro-biomass/train/ID105271783.jpg\n/kaggle/input/csiro-biomass/train/ID1703304524.jpg\n/kaggle/input/csiro-biomass/train/ID875119737.jpg\n/kaggle/input/csiro-biomass/train/ID1176292407.jpg\n/kaggle/input/csiro-biomass/train/ID1729002155.jpg\n/kaggle/input/csiro-biomass/train/ID2091439402.jpg\n/kaggle/input/csiro-biomass/train/ID576137678.jpg\n/kaggle/input/csiro-biomass/train/ID1946311744.jpg\n/kaggle/input/csiro-biomass/train/ID1982662138.jpg\n/kaggle/input/csiro-biomass/train/ID983582017.jpg\n/kaggle/input/csiro-biomass/train/ID661817669.jpg\n/kaggle/input/csiro-biomass/train/ID753699705.jpg\n/kaggle/input/csiro-biomass/train/ID1789834546.jpg\n/kaggle/input/csiro-biomass/train/ID529933668.jpg\n/kaggle/input/csiro-biomass/train/ID490139972.jpg\n/kaggle/input/csiro-biomass/train/ID743847993.jpg\n/kaggle/input/csiro-biomass/train/ID7850481.jpg\n/kaggle/input/csiro-biomass/train/ID1088965591.jpg\n/kaggle/input/csiro-biomass/train/ID629980789.jpg\n/kaggle/input/csiro-biomass/train/ID1119739385.jpg\n/kaggle/input/csiro-biomass/train/ID1477176296.jpg\n/kaggle/input/csiro-biomass/train/ID1113121340.jpg\n/kaggle/input/csiro-biomass/train/ID2131261930.jpg\n/kaggle/input/csiro-biomass/train/ID2145635095.jpg\n/kaggle/input/csiro-biomass/train/ID1414371018.jpg\n/kaggle/input/csiro-biomass/train/ID1148666289.jpg\n/kaggle/input/csiro-biomass/train/ID839432753.jpg\n/kaggle/input/csiro-biomass/train/ID157479394.jpg\n/kaggle/input/csiro-biomass/train/ID1761544403.jpg\n/kaggle/input/csiro-biomass/train/ID846984946.jpg\n/kaggle/input/csiro-biomass/train/ID751517087.jpg\n/kaggle/input/csiro-biomass/train/ID577112774.jpg\n/kaggle/input/csiro-biomass/train/ID353997899.jpg\n/kaggle/input/csiro-biomass/train/ID748979397.jpg\n/kaggle/input/csiro-biomass/train/ID1070112260.jpg\n/kaggle/input/csiro-biomass/train/ID1108283583.jpg\n/kaggle/input/csiro-biomass/train/ID1868719645.jpg\n/kaggle/input/csiro-biomass/train/ID1980675327.jpg\n/kaggle/input/csiro-biomass/train/ID1163061745.jpg\n/kaggle/input/csiro-biomass/train/ID1148528732.jpg\n/kaggle/input/csiro-biomass/train/ID534966093.jpg\n/kaggle/input/csiro-biomass/train/ID1717006117.jpg\n/kaggle/input/csiro-biomass/train/ID1953218650.jpg\n/kaggle/input/csiro-biomass/train/ID633775166.jpg\n/kaggle/input/csiro-biomass/train/ID808093827.jpg\n/kaggle/input/csiro-biomass/train/ID1997244125.jpg\n/kaggle/input/csiro-biomass/train/ID1920959057.jpg\n/kaggle/input/csiro-biomass/train/ID1948354837.jpg\n/kaggle/input/csiro-biomass/train/ID364856705.jpg\n/kaggle/input/csiro-biomass/train/ID249042826.jpg\n/kaggle/input/csiro-biomass/train/ID332742639.jpg\n/kaggle/input/csiro-biomass/train/ID1680597197.jpg\n/kaggle/input/csiro-biomass/train/ID1421714468.jpg\n/kaggle/input/csiro-biomass/train/ID905397692.jpg\n/kaggle/input/csiro-biomass/train/ID1782509721.jpg\n/kaggle/input/csiro-biomass/train/ID141370843.jpg\n/kaggle/input/csiro-biomass/train/ID2056982009.jpg\n/kaggle/input/csiro-biomass/train/ID94564238.jpg\n/kaggle/input/csiro-biomass/train/ID8209776.jpg\n/kaggle/input/csiro-biomass/train/ID908524512.jpg\n/kaggle/input/csiro-biomass/train/ID610397481.jpg\n/kaggle/input/csiro-biomass/train/ID750820644.jpg\n/kaggle/input/csiro-biomass/train/ID1515990019.jpg\n/kaggle/input/csiro-biomass/train/ID1547945326.jpg\n/kaggle/input/csiro-biomass/train/ID587125778.jpg\n/kaggle/input/csiro-biomass/train/ID1620371305.jpg\n/kaggle/input/csiro-biomass/train/ID1474775613.jpg\n/kaggle/input/csiro-biomass/train/ID545360459.jpg\n/kaggle/input/csiro-biomass/train/ID1783499590.jpg\n/kaggle/input/csiro-biomass/train/ID1249094008.jpg\n/kaggle/input/csiro-biomass/train/ID1525817840.jpg\n/kaggle/input/csiro-biomass/train/ID227847873.jpg\n/kaggle/input/csiro-biomass/train/ID1052620238.jpg\n/kaggle/input/csiro-biomass/train/ID1888700589.jpg\n/kaggle/input/csiro-biomass/train/ID2052442675.jpg\n/kaggle/input/csiro-biomass/train/ID963903358.jpg\n/kaggle/input/csiro-biomass/train/ID1121692672.jpg\n/kaggle/input/csiro-biomass/train/ID1343327476.jpg\n/kaggle/input/csiro-biomass/train/ID1667778338.jpg\n/kaggle/input/csiro-biomass/train/ID257822026.jpg\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install timm --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T16:58:02.890913Z","iopub.execute_input":"2025-12-21T16:58:02.891113Z","iopub.status.idle":"2025-12-21T16:58:06.097859Z","shell.execute_reply.started":"2025-12-21T16:58:02.891095Z","shell.execute_reply":"2025-12-21T16:58:06.096889Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\"\"\"\nDINOv2 Tiled Training \n==================================================\n\nCopy this entire file into a single Kaggle notebook cell and run.\n\nPrerequisites:\n1. Enable GPU in Kaggle\n2. Enable Internet (for DINOv2 download)\n3. Have CSIRO dataset in /kaggle/input/\n\n\"\"\"\n\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import List, Optional, Tuple, Callable\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm import tqdm\nfrom PIL import Image\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\n\n\n# =============================================================================\n# CONFIGURATION\n# =============================================================================\n\nclass Config:\n    \"\"\"Training configuration.\"\"\"\n    # Auto-detect paths\n    DATA_DIR = None  # Will be auto-detected\n    OUTPUT_DIR = '/kaggle/working/experiments/dinov2_tiled'\n    \n    # Model\n    BACKBONE = 'dinov2_vitl14'  # or 'dinov2_vitb14' for faster\n    USE_GLOBAL_STREAM = True\n    TILE_SIZE = 518\n    FREEZE_BACKBONE = True\n    UNFREEZE_LAST_N_BLOCKS = 0\n    MLP_HIDDEN_DIMS = [1024, 512]\n    DROPOUT = 0.3\n    \n    # Training\n    EPOCHS = 30\n    BATCH_SIZE = 16  # Increased for 2 GPUs (4 per GPU)\n    LR_HEAD = 1e-4\n    LR_BACKBONE = 1e-5\n    WEIGHT_DECAY = 1e-4\n    PATIENCE = 10\n    \n    # Loss\n    TARGET_WEIGHTS = [0.1, 0.1, 0.1, 0.1, 0.5]  # Green, Dead, Clover, GDM, Total\n    CONSTRAINT_LAMBDA = 0.1\n    \n    # CV\n    N_FOLDS = 5\n    FOLD = 0  # Train single fold for Kaggle\n    \n    # Other\n    NUM_WORKERS = 4  # Increased for 2 GPUs\n    SEED = 42\n    USE_MULTI_GPU = True  # Enable multi-GPU training\n    \n    TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n\n\n# =============================================================================\n# DATA TRANSFORMS\n# =============================================================================\n\nclass TileTransform:\n    \"\"\"Split image into 2x2 tiles + optional global.\"\"\"\n    \n    def __init__(self, tile_size=518, include_global=True, augment=False):\n        self.tile_size = tile_size\n        self.include_global = include_global\n        self.augment = augment\n        \n        self.normalize = T.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n        self.resize = T.Resize((tile_size, tile_size), interpolation=T.InterpolationMode.BICUBIC, antialias=True)\n    \n    def _crop_tile(self, image, tile_idx):\n        w, h = image.size\n        half_w, half_h = w // 2, h // 2\n        positions = {\n            0: (0, 0, half_w, half_h),\n            1: (half_w, 0, w, half_h),\n            2: (0, half_h, half_w, h),\n            3: (half_w, half_h, w, h),\n        }\n        return image.crop(positions[tile_idx])\n    \n    def __call__(self, image):\n        if self.augment:\n            if random.random() < 0.5:\n                image = TF.hflip(image)\n            if random.random() < 0.5:\n                image = TF.vflip(image)\n            if random.random() < 0.8:\n                image = T.ColorJitter(0.2, 0.2, 0.2, 0.1)(image)\n        \n        tiles = []\n        for i in range(4):\n            tile = self._crop_tile(image, i)\n            tile = self.resize(tile)\n            tile = T.ToTensor()(tile)\n            tile = self.normalize(tile)\n            tiles.append(tile)\n        \n        if self.include_global:\n            global_img = self.resize(image)\n            global_img = T.ToTensor()(global_img)\n            global_img = self.normalize(global_img)\n            tiles.append(global_img)\n        \n        return torch.stack(tiles, dim=0)\n\n\n# =============================================================================\n# DATASET\n# =============================================================================\n\nclass PastureTiledDataset(Dataset):\n    \"\"\"Dataset for pasture images with tiling.\"\"\"\n    \n    def __init__(self, df, data_dir, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.data_dir = Path(data_dir)\n        self.transform = transform\n        self.target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = self.data_dir / row['image_path']\n        image = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            tiles = self.transform(image)\n        else:\n            tiles = torch.zeros(5, 3, 518, 518)  # dummy\n        \n        targets = torch.tensor([row[col] for col in self.target_cols], dtype=torch.float32)\n        return tiles, targets\n\n\ndef load_train_data(data_dir):\n    \"\"\"Load and reshape training data.\"\"\"\n    df_long = pd.read_csv(Path(data_dir) / 'train.csv')\n    df_long['image_id'] = df_long['sample_id'].str.split('__').str[0]\n    \n    df_wide = df_long.pivot(index='image_id', columns='target_name', values='target').reset_index()\n    image_paths = df_long.groupby('image_id')['image_path'].first()\n    df_wide['image_path'] = df_wide['image_id'].map(image_paths)\n    \n    target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n    return df_wide[['image_id', 'image_path'] + target_cols]\n\n\n# =============================================================================\n# MODEL\n# =============================================================================\n\nclass TiledDinoModel(nn.Module):\n    \"\"\"DINOv2 with tiled input processing.\"\"\"\n    \n    def __init__(self, backbone_name='dinov2_vitl14', num_tiles=5, \n                 mlp_hidden_dims=[1024, 512], dropout=0.3, freeze_backbone=True):\n        super().__init__()\n        \n        self.backbone = torch.hub.load('facebookresearch/dinov2', backbone_name, pretrained=True)\n        self.feature_dim = self.backbone.embed_dim\n        self.num_tiles = num_tiles\n        \n        if freeze_backbone:\n            for param in self.backbone.parameters():\n                param.requires_grad = False\n        \n        # MLP head\n        layers = []\n        prev_dim = num_tiles * self.feature_dim\n        for hidden_dim in mlp_hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.ReLU(inplace=True),\n                nn.Dropout(p=dropout),\n            ])\n            prev_dim = hidden_dim\n        layers.append(nn.Linear(prev_dim, 5))\n        self.head = nn.Sequential(*layers)\n    \n    def forward(self, tiles):\n        batch_size, num_tiles, channels, height, width = tiles.shape\n        tiles_flat = tiles.reshape(batch_size * num_tiles, channels, height, width)\n        \n        with torch.set_grad_enabled(self.training):\n            features = self.backbone(tiles_flat)\n        \n        # Use contiguous().view() or reshape() to handle non-contiguous tensors\n        features = features.contiguous().view(batch_size, num_tiles * self.feature_dim)\n        return self.head(features)\n    \n    def get_trainable_params(self):\n        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n\n\n# =============================================================================\n# LOSSES\n# =============================================================================\n\nclass CombinedLoss(nn.Module):\n    \"\"\"Weighted MSE + physical constraint.\"\"\"\n    \n    def __init__(self, target_weights=[0.1, 0.1, 0.1, 0.1, 0.5], constraint_lambda=0.1):\n        super().__init__()\n        weights = [w / sum(target_weights) for w in target_weights]\n        self.register_buffer('weights', torch.tensor(weights, dtype=torch.float32))\n        self.constraint_lambda = constraint_lambda\n    \n    def forward(self, predictions, targets):\n        # Ensure weights are on the same device as predictions\n        weights = self.weights.to(predictions.device)\n        \n        # Weighted MSE\n        mse_per_target = (predictions - targets) ** 2\n        weighted_mse = mse_per_target * weights\n        main_loss = weighted_mse.sum(dim=1).mean()\n        \n        # Physical constraint: Total ‚âà Green + Dead + Clover\n        pred_sum = predictions[:, 0] + predictions[:, 1] + predictions[:, 2]\n        pred_total = predictions[:, 4]\n        constraint_loss = ((pred_total - pred_sum) ** 2).mean()\n        \n        total_loss = main_loss + self.constraint_lambda * constraint_loss\n        \n        return {\n            'total': total_loss,\n            'main': main_loss,\n            'constraint': constraint_loss\n        }\n\n\ndef weighted_r2_score(predictions, targets, weights=[0.1, 0.1, 0.1, 0.1, 0.5]):\n    \"\"\"Compute weighted R¬≤ score.\"\"\"\n    target_names = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n    results = {}\n    weighted_r2 = 0.0\n    \n    for i, (name, w) in enumerate(zip(target_names, weights)):\n        pred_i = predictions[:, i]\n        target_i = targets[:, i]\n        \n        ss_res = ((target_i - pred_i) ** 2).sum()\n        ss_tot = ((target_i - target_i.mean()) ** 2).sum()\n        \n        r2 = 1 - (ss_res / ss_tot) if ss_tot > 1e-8 else torch.tensor(0.0)\n        results[name] = r2.item()\n        weighted_r2 += w * r2.item()\n    \n    results['weighted_r2'] = weighted_r2\n    return results\n\n\n# =============================================================================\n# TRAINING FUNCTIONS\n# =============================================================================\n\ndef train_epoch(model, dataloader, criterion, optimizer, device, scaler=None):\n    \"\"\"Train one epoch.\"\"\"\n    model.train()\n    running_loss = 0.0\n    num_samples = 0\n    \n    for tiles, targets in tqdm(dataloader, desc='Training', leave=False):\n        tiles, targets = tiles.to(device), targets.to(device)\n        batch_size = tiles.size(0)\n        \n        optimizer.zero_grad()\n        \n        if scaler is not None:\n            with torch.autocast(device_type='cuda', dtype=torch.float16):\n                outputs = model(tiles)\n                losses = criterion(outputs, targets)\n            scaler.scale(losses['total']).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            outputs = model(tiles)\n            losses = criterion(outputs, targets)\n            losses['total'].backward()\n            optimizer.step()\n        \n        running_loss += losses['total'].item() * batch_size\n        num_samples += batch_size\n    \n    return running_loss / num_samples\n\n\ndef validate(model, dataloader, criterion, device):\n    \"\"\"Validate model.\"\"\"\n    model.eval()\n    running_loss = 0.0\n    num_samples = 0\n    all_predictions = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for tiles, targets in tqdm(dataloader, desc='Validation', leave=False):\n            tiles, targets = tiles.to(device), targets.to(device)\n            batch_size = tiles.size(0)\n            \n            outputs = model(tiles)\n            losses = criterion(outputs, targets)\n            \n            running_loss += losses['total'].item() * batch_size\n            num_samples += batch_size\n            \n            all_predictions.append(outputs.cpu())\n            all_targets.append(targets.cpu())\n    \n    all_predictions = torch.cat(all_predictions, dim=0)\n    all_targets = torch.cat(all_targets, dim=0)\n    \n    r2_scores = weighted_r2_score(all_predictions, all_targets)\n    \n    return {\n        'loss': running_loss / num_samples,\n        'r2': r2_scores\n    }\n\n\n# =============================================================================\n# AUTO-DETECT KAGGLE PATHS\n# =============================================================================\n\ndef find_csiro_data_dir():\n    \"\"\"Find CSIRO dataset in Kaggle input.\"\"\"\n    kaggle_input = Path('/kaggle/input')\n    \n    if not kaggle_input.exists():\n        return '../csiro-biomass'\n    \n    for dirname, _, filenames in os.walk(kaggle_input):\n        if 'train.csv' in filenames and 'test.csv' in filenames:\n            print(f\"‚úì Found CSIRO dataset at: {dirname}\")\n            return dirname\n    \n    raise FileNotFoundError(\"Could not find CSIRO dataset in /kaggle/input\")\n\n\n# =============================================================================\n# MAIN TRAINING\n# =============================================================================\n\ndef main():\n    \"\"\"Main training function.\"\"\"\n    config = Config()\n    \n    # Set seed\n    random.seed(config.SEED)\n    np.random.seed(config.SEED)\n    torch.manual_seed(config.SEED)\n    torch.cuda.manual_seed_all(config.SEED)\n    \n    # Get device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    n_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"üåø DINOv2 Tiled Model - Kaggle Training\")\n    print(f\"{'='*80}\")\n    print(f\"Device: {device}\")\n    if n_gpus > 1:\n        print(f\"üöÄ Multi-GPU: {n_gpus} GPUs detected!\")\n        for i in range(n_gpus):\n            print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n    elif n_gpus == 1:\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    \n    # Find data\n    if config.DATA_DIR is None:\n        config.DATA_DIR = find_csiro_data_dir()\n    print(f\"Data: {config.DATA_DIR}\")\n    \n    # Create output dir\n    Path(config.OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n    \n    # Load data\n    print(\"\\nüìÇ Loading data...\")\n    df = load_train_data(config.DATA_DIR)\n    print(f\"‚úì Total images: {len(df)}\")\n    \n    # Setup CV\n    gkf = GroupKFold(n_splits=config.N_FOLDS)\n    groups = df['image_id'].values\n    \n    # Get fold split\n    for fold_idx, (train_idx, val_idx) in enumerate(gkf.split(df, groups=groups)):\n        if fold_idx != config.FOLD:\n            continue\n        \n        train_df = df.iloc[train_idx]\n        val_df = df.iloc[val_idx]\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"FOLD {fold_idx + 1}/{config.N_FOLDS}\")\n        print(f\"{'='*80}\")\n        print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}\")\n        \n        # Create datasets\n        train_transform = TileTransform(config.TILE_SIZE, config.USE_GLOBAL_STREAM, augment=True)\n        val_transform = TileTransform(config.TILE_SIZE, config.USE_GLOBAL_STREAM, augment=False)\n        \n        train_dataset = PastureTiledDataset(train_df, config.DATA_DIR, train_transform)\n        val_dataset = PastureTiledDataset(val_df, config.DATA_DIR, val_transform)\n        \n        train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, \n                                  num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True)\n        val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False,\n                               num_workers=config.NUM_WORKERS, pin_memory=True)\n        \n        # Create model\n        print(\"\\nüì• Loading DINOv2 model (first time downloads ~1.2GB)...\")\n        num_tiles = 5 if config.USE_GLOBAL_STREAM else 4\n        model = TiledDinoModel(\n            backbone_name=config.BACKBONE,\n            num_tiles=num_tiles,\n            mlp_hidden_dims=config.MLP_HIDDEN_DIMS,\n            dropout=config.DROPOUT,\n            freeze_backbone=config.FREEZE_BACKBONE\n        ).to(device)\n        \n        # Multi-GPU support\n        n_gpus = torch.cuda.device_count()\n        if config.USE_MULTI_GPU and n_gpus > 1:\n            print(f\"‚úì Using {n_gpus} GPUs with DataParallel!\")\n            model = nn.DataParallel(model)\n        \n        # Get trainable params (handle DataParallel wrapper)\n        base_model = model.module if hasattr(model, 'module') else model\n        print(f\"‚úì Model loaded!\")\n        print(f\"GPUs: {n_gpus}\")\n        print(f\"Trainable params: {base_model.get_trainable_params():,}\")\n        \n        # Setup training\n        criterion = CombinedLoss(config.TARGET_WEIGHTS, config.CONSTRAINT_LAMBDA)\n        optimizer = optim.AdamW(model.parameters(), lr=config.LR_HEAD, weight_decay=config.WEIGHT_DECAY)\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.EPOCHS, eta_min=1e-6)\n        scaler = torch.GradScaler('cuda') if device.type == 'cuda' else None\n        \n        # Training loop\n        best_r2 = float('-inf')\n        patience_counter = 0\n        history = []\n        \n        fold_dir = Path(config.OUTPUT_DIR) / f'fold_{fold_idx}'\n        fold_dir.mkdir(parents=True, exist_ok=True)\n        \n        print(f\"\\nüöÄ Starting training...\")\n        \n        for epoch in range(config.EPOCHS):\n            print(f\"\\nEpoch {epoch + 1}/{config.EPOCHS}\")\n            \n            # Train\n            train_loss = train_epoch(model, train_loader, criterion, optimizer, device, scaler)\n            \n            # Validate\n            val_results = validate(model, val_loader, criterion, device)\n            \n            scheduler.step()\n            \n            weighted_r2 = val_results['r2']['weighted_r2']\n            \n            print(f\"  Train Loss: {train_loss:.4f}\")\n            print(f\"  Val Loss: {val_results['loss']:.4f}\")\n            print(f\"  Weighted R¬≤: {weighted_r2:.4f}\")\n            \n            history.append({\n                'epoch': epoch + 1,\n                'train_loss': train_loss,\n                'val_loss': val_results['loss'],\n                'weighted_r2': weighted_r2\n            })\n            \n            # Save best model (handle DataParallel wrapper)\n            if weighted_r2 > best_r2:\n                best_r2 = weighted_r2\n                patience_counter = 0\n                # Save underlying model if using DataParallel\n                model_to_save = model.module if hasattr(model, 'module') else model\n                torch.save(model_to_save.state_dict(), fold_dir / 'best_model.pth')\n                print(f\"  ‚úì Saved best model (R¬≤: {best_r2:.4f})\")\n            else:\n                patience_counter += 1\n                print(f\"  ‚è≥ Patience: {patience_counter}/{config.PATIENCE}\")\n            \n            if patience_counter >= config.PATIENCE:\n                print(f\"\\n‚ö†Ô∏è Early stopping at epoch {epoch + 1}\")\n                break\n        \n        # Save history\n        pd.DataFrame(history).to_csv(fold_dir / 'history.csv', index=False)\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"üéâ TRAINING COMPLETE\")\n        print(f\"Best Weighted R¬≤: {best_r2:.4f}\")\n        print(f\"Models saved to: {fold_dir}\")\n        print(f\"{'='*80}\\n\")\n        \n        break  # Only train one fold\n\n\n# =============================================================================\n# RUN\n# =============================================================================\n\nif __name__ == '__main__':\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T17:03:05.588319Z","iopub.execute_input":"2025-12-21T17:03:05.588648Z","iopub.status.idle":"2025-12-21T18:08:13.798264Z","shell.execute_reply.started":"2025-12-21T17:03:05.588615Z","shell.execute_reply":"2025-12-21T18:08:13.797559Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nüåø DINOv2 Tiled Model - Kaggle Training\n================================================================================\nDevice: cuda\nüöÄ Multi-GPU: 2 GPUs detected!\n   GPU 0: Tesla T4\n   GPU 1: Tesla T4\n‚úì Found CSIRO dataset at: /kaggle/input/csiro-biomass\nData: /kaggle/input/csiro-biomass\n\nüìÇ Loading data...\n‚úì Total images: 357\n\n================================================================================\nFOLD 1/5\n================================================================================\nTrain size: 285, Val size: 72\n\nüì• Loading DINOv2 model (first time downloads ~1.2GB)...\n","output_type":"stream"},{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n","output_type":"stream"},{"name":"stdout","text":"‚úì Using 2 GPUs with DataParallel!\n‚úì Model loaded!\nGPUs: 2\nTrainable params: 5,771,269\n\nüöÄ Starting training...\n\nEpoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 1441.5445\n  Val Loss: 812.7193\n  Weighted R¬≤: -0.1469\n  ‚úì Saved best model (R¬≤: -0.1469)\n\nEpoch 2/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 575.2232\n  Val Loss: 412.5744\n  Weighted R¬≤: 0.2901\n  ‚úì Saved best model (R¬≤: 0.2901)\n\nEpoch 3/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 343.1190\n  Val Loss: 248.9936\n  Weighted R¬≤: 0.4717\n  ‚úì Saved best model (R¬≤: 0.4717)\n\nEpoch 4/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 279.2047\n  Val Loss: 223.0090\n  Weighted R¬≤: 0.5085\n  ‚úì Saved best model (R¬≤: 0.5085)\n\nEpoch 5/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 235.9704\n  Val Loss: 209.2273\n  Weighted R¬≤: 0.5390\n  ‚úì Saved best model (R¬≤: 0.5390)\n\nEpoch 6/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 223.7717\n  Val Loss: 204.4057\n  Weighted R¬≤: 0.5524\n  ‚úì Saved best model (R¬≤: 0.5524)\n\nEpoch 7/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 198.3090\n  Val Loss: 185.2241\n  Weighted R¬≤: 0.5742\n  ‚úì Saved best model (R¬≤: 0.5742)\n\nEpoch 8/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 178.7167\n  Val Loss: 165.9990\n  Weighted R¬≤: 0.5969\n  ‚úì Saved best model (R¬≤: 0.5969)\n\nEpoch 9/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 188.8049\n  Val Loss: 164.8145\n  Weighted R¬≤: 0.6021\n  ‚úì Saved best model (R¬≤: 0.6021)\n\nEpoch 10/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 160.8480\n  Val Loss: 166.4721\n  Weighted R¬≤: 0.6057\n  ‚úì Saved best model (R¬≤: 0.6057)\n\nEpoch 11/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 165.5774\n  Val Loss: 164.6655\n  Weighted R¬≤: 0.6059\n  ‚úì Saved best model (R¬≤: 0.6059)\n\nEpoch 12/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 143.9964\n  Val Loss: 154.6870\n  Weighted R¬≤: 0.6222\n  ‚úì Saved best model (R¬≤: 0.6222)\n\nEpoch 13/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 142.3198\n  Val Loss: 160.6433\n  Weighted R¬≤: 0.6106\n  ‚è≥ Patience: 1/10\n\nEpoch 14/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 154.2932\n  Val Loss: 160.7528\n  Weighted R¬≤: 0.6192\n  ‚è≥ Patience: 2/10\n\nEpoch 15/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 131.7420\n  Val Loss: 151.0268\n  Weighted R¬≤: 0.6282\n  ‚úì Saved best model (R¬≤: 0.6282)\n\nEpoch 16/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 139.9056\n  Val Loss: 154.2638\n  Weighted R¬≤: 0.6239\n  ‚è≥ Patience: 1/10\n\nEpoch 17/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 136.0007\n  Val Loss: 151.7536\n  Weighted R¬≤: 0.6296\n  ‚úì Saved best model (R¬≤: 0.6296)\n\nEpoch 18/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 125.2598\n  Val Loss: 146.5788\n  Weighted R¬≤: 0.6385\n  ‚úì Saved best model (R¬≤: 0.6385)\n\nEpoch 19/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 115.8955\n  Val Loss: 150.8929\n  Weighted R¬≤: 0.6317\n  ‚è≥ Patience: 1/10\n\nEpoch 20/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 125.4977\n  Val Loss: 146.6446\n  Weighted R¬≤: 0.6382\n  ‚è≥ Patience: 2/10\n\nEpoch 21/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 117.4712\n  Val Loss: 145.0942\n  Weighted R¬≤: 0.6414\n  ‚úì Saved best model (R¬≤: 0.6414)\n\nEpoch 22/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 109.7374\n  Val Loss: 145.8002\n  Weighted R¬≤: 0.6426\n  ‚úì Saved best model (R¬≤: 0.6426)\n\nEpoch 23/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 116.6319\n  Val Loss: 143.6262\n  Weighted R¬≤: 0.6438\n  ‚úì Saved best model (R¬≤: 0.6438)\n\nEpoch 24/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 119.9150\n  Val Loss: 141.7455\n  Weighted R¬≤: 0.6457\n  ‚úì Saved best model (R¬≤: 0.6457)\n\nEpoch 25/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 104.3799\n  Val Loss: 142.2087\n  Weighted R¬≤: 0.6464\n  ‚úì Saved best model (R¬≤: 0.6464)\n\nEpoch 26/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 100.9775\n  Val Loss: 142.1972\n  Weighted R¬≤: 0.6462\n  ‚è≥ Patience: 1/10\n\nEpoch 27/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 104.5563\n  Val Loss: 141.9162\n  Weighted R¬≤: 0.6463\n  ‚è≥ Patience: 2/10\n\nEpoch 28/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 103.7680\n  Val Loss: 141.9412\n  Weighted R¬≤: 0.6462\n  ‚è≥ Patience: 3/10\n\nEpoch 29/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 102.8773\n  Val Loss: 141.8012\n  Weighted R¬≤: 0.6467\n  ‚úì Saved best model (R¬≤: 0.6467)\n\nEpoch 30/30\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 95.2695\n  Val Loss: 141.7439\n  Weighted R¬≤: 0.6468\n  ‚úì Saved best model (R¬≤: 0.6468)\n\n================================================================================\nüéâ TRAINING COMPLETE\nBest Weighted R¬≤: 0.6468\nModels saved to: /kaggle/working/experiments/dinov2_tiled/fold_0\n================================================================================\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}