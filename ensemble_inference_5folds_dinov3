{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"},{"sourceId":287733923,"sourceType":"kernelVersion"},{"sourceId":287736779,"sourceType":"kernelVersion"},{"sourceId":4535,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3327,"modelId":986},{"sourceId":698247,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":529647,"modelId":543644}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input/csiro-biomass/test'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-22T07:21:35.758265Z","iopub.execute_input":"2025-12-22T07:21:35.758973Z","iopub.status.idle":"2025-12-22T07:21:35.763827Z","shell.execute_reply.started":"2025-12-22T07:21:35.758941Z","shell.execute_reply":"2025-12-22T07:21:35.763133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# \"\"\"\n# Create a placeholder submission.csv for CSIRO Biomass Competition\n# \"\"\"\n\n# import pandas as pd\n\n# # Competition data directory\n# DATA_DIR = '/kaggle/input/csiro-biomass'\n\n# # Load test.csv to get the correct sample_ids\n# test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n\n# # Create submission with placeholder values (zeros)\n# submission = pd.DataFrame({\n#     'sample_id': test_df['sample_id'],\n#     'target': 0.0  # Placeholder value\n# })\n\n# # Save submission\n# submission.to_csv('submission.csv', index=False)\n\n# print(\"âœ… Created submission.csv with placeholder values\")\n# print(f\"   Shape: {submission.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T14:59:57.327252Z","iopub.execute_input":"2025-12-25T14:59:57.328899Z","iopub.status.idle":"2025-12-25T14:59:57.360604Z","shell.execute_reply.started":"2025-12-25T14:59:57.328856Z","shell.execute_reply":"2025-12-25T14:59:57.359451Z"}},"outputs":[{"name":"stdout","text":"âœ… Created submission.csv with placeholder values\n   Shape: (5, 2)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install torchvision\n!pip install torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T07:16:16.519170Z","iopub.execute_input":"2025-12-22T07:16:16.519715Z","iopub.status.idle":"2025-12-22T07:16:24.733473Z","shell.execute_reply.started":"2025-12-22T07:16:16.519685Z","shell.execute_reply":"2025-12-22T07:16:24.732732Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nEnsemble Inference for CSIRO Biomass - Kaggle FULL OFFLINE Version\n====================================================================\nJust run this cell! No internet required.\n\nSETUP:\n1. Upload your DINOv2 backbone (dinov2_vitl14.pth) as a Kaggle dataset\n2. Upload your trained models as a Kaggle dataset\n3. Edit the paths below if needed\n4. Run the cell!\n\nTo save the backbone (run once with internet):\n    import torch\n    backbone = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14', pretrained=True)\n    torch.save(backbone, 'dinov2_vitl14.pth')\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom tqdm import tqdm\nfrom pathlib import Path\n\n# =============================================================================\n# CONFIGURATION - Edit these paths for your Kaggle setup\n# =============================================================================\n\n# Path to your uploaded DINOv2 backbone file\nBACKBONE_PATH = '/kaggle/input/dinov2-backbone/dinov2_vitl14.pth'\n\n# Path to your trained model checkpoints  \nMODELS_DIR = '/kaggle/input/dinov2-logtransform/pytorch/default/1'\n\n# Competition data directory\nDATA_DIR = '/kaggle/input/csiro-biomass'\n\n# Output settings\nOUTPUT_PATH = 'submission.csv'\nTTA_AUGS = 8  # Number of TTA augmentations\nBATCH_SIZE = 8\nNUM_WORKERS = 4\nUSE_LOG_TRANSFORM = True  # Models were trained with log transform\n\n# =============================================================================\n# MODEL ARCHITECTURE (Must match training exactly)\n# =============================================================================\n\nclass TiledDinoModel(nn.Module):\n    \"\"\"DINOv2 with tiled input processing - OFFLINE VERSION.\"\"\"\n    \n    def __init__(self, backbone, num_tiles=5, mlp_hidden_dims=[1024, 512], \n                 dropout=0.4, use_attention=False, num_attention_heads=8):\n        super().__init__()\n        \n        # Use pre-loaded backbone (no internet needed)\n        self.backbone = backbone\n        self.feature_dim = self.backbone.embed_dim\n        self.num_tiles = num_tiles\n        self.use_attention = use_attention\n        \n        # Freeze backbone for inference\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n        \n        # Tile position embeddings\n        self.tile_pos_embed = nn.Parameter(torch.randn(1, num_tiles, self.feature_dim) * 0.02)\n        \n        # Multi-head attention between tiles (if enabled)\n        if self.use_attention:\n            self.tile_attention = nn.MultiheadAttention(\n                embed_dim=self.feature_dim,\n                num_heads=num_attention_heads,\n                dropout=dropout,\n                batch_first=True\n            )\n            self.attention_norm = nn.LayerNorm(self.feature_dim)\n        \n        # MLP head\n        layers = []\n        prev_dim = num_tiles * self.feature_dim\n        for hidden_dim in mlp_hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.ReLU(inplace=True),\n                nn.Dropout(p=dropout),\n            ])\n            prev_dim = hidden_dim\n        layers.append(nn.Linear(prev_dim, 5))\n        self.head = nn.Sequential(*layers)\n    \n    def forward(self, tiles):\n        batch_size, num_tiles, channels, height, width = tiles.shape\n        tiles_flat = tiles.reshape(batch_size * num_tiles, channels, height, width)\n        \n        with torch.no_grad():\n            features = self.backbone(tiles_flat)\n        \n        features = features.view(batch_size, num_tiles, self.feature_dim)\n        features = features + self.tile_pos_embed\n        \n        if self.use_attention:\n            attn_out, _ = self.tile_attention(features, features, features)\n            features = self.attention_norm(features + attn_out)\n        \n        features = features.view(batch_size, num_tiles * self.feature_dim)\n        return self.head(features)\n\n\n# =============================================================================\n# DATA TRANSFORMS\n# =============================================================================\n\nclass TileTransform:\n    \"\"\"Split image into 2x2 tiles + global.\"\"\"\n    \n    def __init__(self, tile_size=518, include_global=True):\n        self.tile_size = tile_size\n        self.include_global = include_global\n        self.normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        self.resize = T.Resize((tile_size, tile_size), interpolation=T.InterpolationMode.BICUBIC, antialias=True)\n    \n    def _crop_tile(self, image, tile_idx):\n        w, h = image.size\n        mid_w, mid_h = w // 2, h // 2\n        crops = [(0, 0, mid_w, mid_h), (mid_w, 0, w, mid_h), (0, mid_h, mid_w, h), (mid_w, mid_h, w, h)]\n        return image.crop(crops[tile_idx])\n    \n    def __call__(self, image):\n        tiles = []\n        for i in range(4):\n            tile = self._crop_tile(image, i)\n            tile = self.resize(tile)\n            tile = T.ToTensor()(tile)\n            tile = self.normalize(tile)\n            tiles.append(tile)\n        \n        if self.include_global:\n            global_img = self.resize(image)\n            global_img = T.ToTensor()(global_img)\n            global_img = self.normalize(global_img)\n            tiles.append(global_img)\n        \n        return torch.stack(tiles, dim=0)\n\n\nclass TTATransform:\n    \"\"\"TTA augmentations for inference.\"\"\"\n    \n    def __init__(self, tile_size=518, include_global=True):\n        self.tile_size = tile_size\n        self.include_global = include_global\n        self.normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        self.resize = T.Resize((tile_size, tile_size), interpolation=T.InterpolationMode.BICUBIC, antialias=True)\n    \n    def _crop_tile(self, image, tile_idx):\n        w, h = image.size\n        mid_w, mid_h = w // 2, h // 2\n        crops = [(0, 0, mid_w, mid_h), (mid_w, 0, w, mid_h), (0, mid_h, mid_w, h), (mid_w, mid_h, w, h)]\n        return image.crop(crops[tile_idx])\n    \n    def generate_augments(self, image, num_augs=8):\n        augments = []\n        tta_configs = [\n            {'flip_h': False, 'flip_v': False, 'rotate': 0},\n            {'flip_h': True, 'flip_v': False, 'rotate': 0},\n            {'flip_h': False, 'flip_v': True, 'rotate': 0},\n            {'flip_h': True, 'flip_v': True, 'rotate': 0},\n            {'flip_h': False, 'flip_v': False, 'rotate': 90},\n            {'flip_h': False, 'flip_v': False, 'rotate': 180},\n            {'flip_h': False, 'flip_v': False, 'rotate': 270},\n            {'flip_h': True, 'flip_v': False, 'rotate': 90},\n        ]\n        \n        for cfg in tta_configs[:num_augs]:\n            aug_img = image.copy()\n            if cfg['flip_h']:\n                aug_img = aug_img.transpose(Image.FLIP_LEFT_RIGHT)\n            if cfg['flip_v']:\n                aug_img = aug_img.transpose(Image.FLIP_TOP_BOTTOM)\n            if cfg['rotate'] > 0:\n                aug_img = aug_img.rotate(cfg['rotate'], expand=False)\n            \n            tiles = []\n            for j in range(4):\n                tile = self._crop_tile(aug_img, j)\n                tile = self.resize(tile)\n                tile = T.ToTensor()(tile)\n                tile = self.normalize(tile)\n                tiles.append(tile)\n            \n            if self.include_global:\n                global_img = self.resize(aug_img)\n                global_img = T.ToTensor()(global_img)\n                global_img = self.normalize(global_img)\n                tiles.append(global_img)\n            \n            augments.append(torch.stack(tiles, dim=0))\n        \n        return torch.stack(augments, dim=0)\n\n\n# =============================================================================\n# DATASET\n# =============================================================================\n\nclass BiomassTestDataset(Dataset):\n    def __init__(self, df, data_dir, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.data_dir = Path(data_dir)\n        self.transform = transform\n        self.image_ids = self.df['sample_id'].str.split('__').str[0].unique()\n        df_temp = self.df.copy()\n        df_temp['image_id'] = df_temp['sample_id'].str.split('__').str[0]\n        self.image_paths = df_temp.drop_duplicates('image_id').set_index('image_id')['image_path'].to_dict()\n    \n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        img_path = self.data_dir / self.image_paths[image_id]\n        with Image.open(img_path) as img:\n            image = img.convert('RGB')\n        tiles = self.transform(image) if self.transform else torch.zeros(5, 3, 518, 518)\n        return tiles, image_id\n\n\n# =============================================================================\n# INFERENCE FUNCTIONS\n# =============================================================================\n\ndef load_model_offline(checkpoint_path, backbone, device):\n    \"\"\"Load model with pre-loaded backbone (OFFLINE).\"\"\"\n    print(f\"  Loading: {checkpoint_path}\")\n    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n    \n    config = checkpoint.get('config', None)\n    mlp_hidden_dims = [1024, 512]\n    dropout = 0.4\n    use_attention = False\n    \n    if config:\n        mlp_hidden_dims = getattr(config, 'MLP_HIDDEN_DIMS', mlp_hidden_dims)\n        dropout = getattr(config, 'DROPOUT', dropout)\n        use_attention = getattr(config, 'USE_TILE_ATTENTION', use_attention)\n    \n    # Create model with pre-loaded backbone\n    model = TiledDinoModel(\n        backbone=backbone,\n        num_tiles=5,\n        mlp_hidden_dims=mlp_hidden_dims,\n        dropout=dropout,\n        use_attention=use_attention,\n    )\n    \n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.to(device)\n    model.eval()\n    \n    best_r2 = checkpoint.get('best_r2', None)\n    if best_r2:\n        print(f\"  âœ“ Loaded | Best RÂ²: {best_r2:.4f}\")\n    else:\n        print(f\"  âœ“ Loaded\")\n    \n    return model\n\n\ndef predict_with_tta(model, dataloader, device, num_tta_augs=8):\n    \"\"\"Generate predictions with TTA.\"\"\"\n    model.eval()\n    all_predictions = []\n    all_image_ids = []\n    tta_transform = TTATransform(tile_size=518, include_global=True)\n    \n    with torch.no_grad():\n        for tiles, image_ids in tqdm(dataloader, desc='  Predicting', leave=False):\n            tiles = tiles.to(device, non_blocking=True)\n            batch_size = tiles.size(0)\n            \n            with torch.amp.autocast('cuda', enabled=True):\n                all_pred = []\n                for i in range(batch_size):\n                    img_id = image_ids[i]\n                    img_path = dataloader.dataset.data_dir / dataloader.dataset.image_paths[img_id]\n                    \n                    with Image.open(img_path) as img:\n                        aug_batch = tta_transform.generate_augments(img.convert('RGB'), num_tta_augs)\n                    \n                    aug_batch = aug_batch.to(device, non_blocking=True)\n                    pred_augs = model(aug_batch)\n                    pred_mean = pred_augs.mean(dim=0)\n                    all_pred.append(pred_mean)\n                \n                outputs = torch.stack(all_pred, dim=0)\n            \n            all_predictions.append(outputs.cpu())\n            all_image_ids.extend(image_ids)\n    \n    return torch.cat(all_predictions, dim=0), all_image_ids\n\n\n# =============================================================================\n# MAIN INFERENCE (FULLY OFFLINE)\n# =============================================================================\n\nprint(\"=\" * 60)\nprint(\"ðŸŒ¿ CSIRO Biomass - DINOv2 Ensemble Inference (OFFLINE)\")\nprint(\"=\" * 60)\n\ndevice = torch.device('cuda')\nprint(f\"Device: {device}\")\nprint(f\"Backbone: {BACKBONE_PATH}\")\nprint(f\"Models: {MODELS_DIR}\")\nprint(f\"Data: {DATA_DIR}\")\nprint(f\"TTA: {TTA_AUGS} augmentations\")\n\n# Load backbone ONCE (offline - no internet needed!)\nprint(f\"\\nðŸ“¦ Loading DINOv2 backbone from local file...\")\nbackbone = torch.load(BACKBONE_PATH, map_location=device, weights_only=False)\nbackbone.eval()\nfor param in backbone.parameters():\n    param.requires_grad = False\nprint(f\"   âœ“ Backbone loaded: embed_dim={backbone.embed_dim}\")\n\n# Find all model checkpoints\nmodels_path = Path(MODELS_DIR)\nmodel_files = sorted(models_path.glob('best_model*.pth'))\nprint(f\"\\nðŸ“‚ Found {len(model_files)} models:\")\nfor f in model_files:\n    print(f\"   - {f.name}\")\n\n# Load test data\ntest_csv = f\"{DATA_DIR}/test.csv\"\nprint(f\"\\nðŸ“‹ Loading test data from: {test_csv}\")\ntest_df = pd.read_csv(test_csv)\nprint(f\"   Test samples: {len(test_df)}\")\nprint(f\"   Unique images: {test_df['sample_id'].str.split('__').str[0].nunique()}\")\n\n# Create dataloader\ntransform = TileTransform(tile_size=518, include_global=True)\ntest_dataset = BiomassTestDataset(test_df, DATA_DIR, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, \n                         num_workers=NUM_WORKERS, pin_memory=True)\n\n# Generate predictions for each model\nall_fold_predictions = []\n\nfor model_file in model_files:\n    print(f\"\\n{'='*60}\")\n    print(f\"ðŸ“¦ Processing {model_file.name}...\")\n    \n    # Load model with SHARED backbone (memory efficient!)\n    model = load_model_offline(model_file, backbone, device)\n    predictions, image_ids = predict_with_tta(model, test_loader, device, num_tta_augs=TTA_AUGS)\n    all_fold_predictions.append(predictions)\n    \n    # Only delete head, keep backbone\n    del model\n    torch.cuda.empty_cache()\n\n# Ensemble: average all predictions\nprint(f\"\\n{'='*60}\")\nprint(f\"âœ… Ensembling {len(all_fold_predictions)} models...\")\nstacked = torch.stack(all_fold_predictions, dim=0)\nensemble_preds = stacked.mean(dim=0).numpy()\n\n# Convert from log-space\nif USE_LOG_TRANSFORM:\n    ensemble_preds = np.expm1(ensemble_preds)\n    print(\"   âœ“ Converted from log-space (expm1)\")\n\n# Clip negative predictions\nensemble_preds = np.clip(ensemble_preds, 0, None)\n\n# Create submission\ntarget_names = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\nrows = []\nfor img_id, preds in zip(image_ids, ensemble_preds):\n    for idx, target_name in enumerate(target_names):\n        sample_id = f\"{img_id}__{target_name}\"\n        rows.append({'sample_id': sample_id, 'target': preds[idx]})\n\nsubmission = pd.DataFrame(rows)\nsubmission.to_csv(OUTPUT_PATH, index=False)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"ðŸ“„ Submission saved to: {OUTPUT_PATH}\")\nprint(f\"   Shape: {submission.shape}\")\nprint(f\"\\nFirst 10 rows:\")\nprint(submission.head(10))\n\nprint(f\"\\nPrediction statistics:\")\nprint(f\"{'Target':<15} {'Min':>10} {'Max':>10} {'Mean':>10} {'Std':>10}\")\nprint(\"-\" * 55)\nfor i, name in enumerate(target_names):\n    col_preds = ensemble_preds[:, i]\n    print(f\"{name:<15} {col_preds.min():>10.2f} {col_preds.max():>10.2f} {col_preds.mean():>10.2f} {col_preds.std():>10.2f}\")\n\nprint(f\"\\n{'='*60}\")\nprint(\"ðŸŽ‰ INFERENCE COMPLETE!\")\nprint(f\"{'='*60}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T07:19:57.583574Z","iopub.execute_input":"2025-12-22T07:19:57.584351Z","iopub.status.idle":"2025-12-22T07:20:05.999459Z","shell.execute_reply.started":"2025-12-22T07:19:57.584300Z","shell.execute_reply":"2025-12-22T07:20:05.998795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}