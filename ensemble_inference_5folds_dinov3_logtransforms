{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a20fe45",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-30T16:12:22.922406Z",
     "iopub.status.busy": "2025-12-30T16:12:22.921946Z",
     "iopub.status.idle": "2025-12-30T16:12:22.925622Z",
     "shell.execute_reply": "2025-12-30T16:12:22.925099Z"
    },
    "papermill": {
     "duration": 0.007978,
     "end_time": "2025-12-30T16:12:22.926908",
     "exception": false,
     "start_time": "2025-12-30T16:12:22.918930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input/csiro-biomass/test'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca12396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:12:22.931059Z",
     "iopub.status.busy": "2025-12-30T16:12:22.930537Z",
     "iopub.status.idle": "2025-12-30T16:12:22.933643Z",
     "shell.execute_reply": "2025-12-30T16:12:22.933124Z"
    },
    "papermill": {
     "duration": 0.006415,
     "end_time": "2025-12-30T16:12:22.934893",
     "exception": false,
     "start_time": "2025-12-30T16:12:22.928478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Create a placeholder submission.csv for CSIRO Biomass Competition\n",
    "# \"\"\"\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Competition data directory\n",
    "# DATA_DIR = '/kaggle/input/csiro-biomass'\n",
    "\n",
    "# # Load test.csv to get the correct sample_ids\n",
    "# test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "# # Create submission with placeholder values (zeros)\n",
    "# submission = pd.DataFrame({\n",
    "#     'sample_id': test_df['sample_id'],\n",
    "#     'target': 0.0  # Placeholder value\n",
    "# })\n",
    "\n",
    "# # Save submission\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# print(\"✅ Created submission.csv with placeholder values\")\n",
    "# print(f\"   Shape: {submission.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ee570a",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-12-30T16:12:22.938768Z",
     "iopub.status.busy": "2025-12-30T16:12:22.938330Z",
     "iopub.status.idle": "2025-12-30T16:12:30.063394Z",
     "shell.execute_reply": "2025-12-30T16:12:30.062662Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 7.129444,
     "end_time": "2025-12-30T16:12:30.065727",
     "exception": false,
     "start_time": "2025-12-30T16:12:22.936283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\r\n",
      "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.8.0+cu126)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (75.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (2025.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.80)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (11.3.0.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (10.3.7.77)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (11.7.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.5.4.2)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (2.27.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (12.6.85)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (1.11.1.6)\r\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision) (3.4.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchvision) (3.0.3)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\r\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae5533f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T16:12:30.071316Z",
     "iopub.status.busy": "2025-12-30T16:12:30.071035Z",
     "iopub.status.idle": "2025-12-30T16:14:13.131614Z",
     "shell.execute_reply": "2025-12-30T16:14:13.130677Z"
    },
    "papermill": {
     "duration": 103.065515,
     "end_time": "2025-12-30T16:14:13.133206",
     "exception": false,
     "start_time": "2025-12-30T16:12:30.067691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Fold 0 ---\n",
      "Building backbone: vit_large_patch14_dinov2\n",
      "Loading weights from: /kaggle/input/dinov2-backbone/dinov2_vitl14.pth\n",
      "Loading trained checkpoint: /kaggle/input/dinov2-logtransform/pytorch/default/1/best_model.pth\n",
      "Note: Ignored unexpected keys: ['tile_pos_embed', 'backbone.mask_token']\n",
      "✓ Trained model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 0 Inference: 100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Fold 1 ---\n",
      "Building backbone: vit_large_patch14_dinov2\n",
      "Loading weights from: /kaggle/input/dinov2-backbone/dinov2_vitl14.pth\n",
      "Loading trained checkpoint: /kaggle/input/dinov2-logtransform/pytorch/default/1/best_model_1.pth\n",
      "Note: Ignored unexpected keys: ['tile_pos_embed', 'backbone.mask_token']\n",
      "✓ Trained model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Inference: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Fold 2 ---\n",
      "Building backbone: vit_large_patch14_dinov2\n",
      "Loading weights from: /kaggle/input/dinov2-backbone/dinov2_vitl14.pth\n",
      "Loading trained checkpoint: /kaggle/input/dinov2-logtransform/pytorch/default/1/best_model_2.pth\n",
      "Note: Ignored unexpected keys: ['tile_pos_embed', 'backbone.mask_token']\n",
      "✓ Trained model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Inference: 100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Fold 3 ---\n",
      "Building backbone: vit_large_patch14_dinov2\n",
      "Loading weights from: /kaggle/input/dinov2-backbone/dinov2_vitl14.pth\n",
      "Loading trained checkpoint: /kaggle/input/dinov2-logtransform/pytorch/default/1/best_model_3.pth\n",
      "Note: Ignored unexpected keys: ['tile_pos_embed', 'backbone.mask_token']\n",
      "✓ Trained model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Inference: 100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Fold 4 ---\n",
      "Building backbone: vit_large_patch14_dinov2\n",
      "Loading weights from: /kaggle/input/dinov2-backbone/dinov2_vitl14.pth\n",
      "Loading trained checkpoint: /kaggle/input/dinov2-logtransform/pytorch/default/1/best_model_4.pth\n",
      "Note: Ignored unexpected keys: ['tile_pos_embed', 'backbone.mask_token']\n",
      "✓ Trained model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4 Inference: 100%|██████████| 1/1 [00:01<00:00,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble Complete! Submission saved to /kaggle/working/submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import timm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "class InferenceConfig:\n",
    "    \"\"\"Inference configuration.\"\"\"\n",
    "    # Paths\n",
    "    DATA_DIR = None  # Auto-detected\n",
    "    CHECKPOINTS = [\n",
    "        '/kaggle/input/dinov2-logtransform/pytorch/default/1/best_model.pth',\n",
    "        '/kaggle/input/dinov2-logtransform/pytorch/default/1/best_model_1.pth',\n",
    "        '/kaggle/input/dinov2-logtransform/pytorch/default/1/best_model_2.pth',\n",
    "        '/kaggle/input/dinov2-logtransform/pytorch/default/1/best_model_3.pth',\n",
    "        '/kaggle/input/dinov2-logtransform/pytorch/default/1/best_model_4.pth',\n",
    "    ]\n",
    "    OUTPUT_PATH = '/kaggle/working/submission.csv'\n",
    "    \n",
    "    # Backbone settings\n",
    "    USE_OFFLINE_BACKBONE = True\n",
    "    OFFLINE_BACKBONE_PATH = '/kaggle/input/dinov2-backbone/dinov2_vitl14.pth'\n",
    "    # Use timm name: 'vit_large_patch14_dinov2' = dinov2_vitl14\n",
    "    BACKBONE = 'vit_large_patch14_dinov2' \n",
    "    \n",
    "    # Model settings\n",
    "    USE_GLOBAL_STREAM = True\n",
    "    TILE_SIZE = 518\n",
    "    MLP_HIDDEN_DIMS = [1024, 512]\n",
    "    DROPOUT = 0.3\n",
    "    \n",
    "    # Inference settings\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 4\n",
    "    USE_TTA = False\n",
    "    APPLY_CONSTRAINT = True\n",
    "    CONSTRAINT_BLEND = 0.3\n",
    "\n",
    "# =============================================================================\n",
    "# DATA TRANSFORMS & DATASET\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    pass\n",
    "\n",
    "class TileTransform:\n",
    "    def __init__(self, tile_size=518, include_global=True):\n",
    "        self.tile_size = tile_size\n",
    "        self.include_global = include_global\n",
    "        self.normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.resize = T.Resize((tile_size, tile_size), interpolation=T.InterpolationMode.BICUBIC, antialias=True)\n",
    "    \n",
    "    def _crop_tile(self, image, tile_idx):\n",
    "        w, h = image.size\n",
    "        hw, hh = w // 2, h // 2\n",
    "        pos = {0: (0, 0, hw, hh), 1: (hw, 0, w, hh), 2: (0, hh, hw, h), 3: (hw, hh, w, h)}\n",
    "        return image.crop(pos[tile_idx])\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        tiles = []\n",
    "        for i in range(4):\n",
    "            tile = self.resize(self._crop_tile(image, i))\n",
    "            tiles.append(self.normalize(T.ToTensor()(tile)))\n",
    "        if self.include_global:\n",
    "            tiles.append(self.normalize(T.ToTensor()(self.resize(image))))\n",
    "        return torch.stack(tiles, dim=0)\n",
    "\n",
    "class PastureTestDataset(Dataset):\n",
    "    def __init__(self, df, data_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self): return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self.data_dir / row['image_path']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        tiles = self.transform(image) if self.transform else torch.zeros(5, 3, 518, 518)\n",
    "        return tiles, row['image_id']\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL ARCHITECTURE (Fixed for timm)\n",
    "# =============================================================================\n",
    "\n",
    "class TiledDinoModel(nn.Module):\n",
    "    def __init__(self, backbone_name, num_tiles=5, mlp_hidden_dims=[1024, 512], \n",
    "                 dropout=0.3, freeze_backbone=True, offline_backbone_path=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(f\"Building backbone: {backbone_name}\")\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=False, num_classes=0)\n",
    "        \n",
    "        if offline_backbone_path and Path(offline_backbone_path).exists():\n",
    "            print(f\"Loading weights from: {offline_backbone_path}\")\n",
    "            sd = torch.load(offline_backbone_path, map_location='cpu', weights_only=True)\n",
    "            self.backbone.load_state_dict(sd, strict=False)\n",
    "        \n",
    "        self.feature_dim = self.backbone.num_features\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters(): p.requires_grad = False\n",
    "            \n",
    "        layers = []\n",
    "        prev_dim = num_tiles * self.feature_dim\n",
    "        for h in mlp_hidden_dims:\n",
    "            layers.extend([nn.Linear(prev_dim, h), nn.ReLU(inplace=True), nn.Dropout(dropout)])\n",
    "            prev_dim = h\n",
    "        layers.append(nn.Linear(prev_dim, 5))\n",
    "        self.head = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, tiles):\n",
    "        # tiles shape: (B, N, C, H, W)\n",
    "        B, N, C, H, W = tiles.shape\n",
    "        \n",
    "        # 1. Flatten for backbone\n",
    "        flat = tiles.view(B * N, C, H, W)\n",
    "        \n",
    "        # 2. Extract features\n",
    "        features = self.backbone(flat) # Expected shape: (B*N, Dim)\n",
    "        \n",
    "        # 3. FIX: Ensure tensor is contiguous before changing shape\n",
    "        # We use .reshape() which is safer than .view() here\n",
    "        features = features.reshape(B, N * self.feature_dim) \n",
    "        \n",
    "        # 4. Pass through MLP head\n",
    "        return self.head(features)\n",
    "\n",
    "# =============================================================================\n",
    "# CORE FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def load_model(checkpoint_path, config, device):\n",
    "    num_tiles = 5 if config.USE_GLOBAL_STREAM else 4\n",
    "    model = TiledDinoModel(\n",
    "        backbone_name=config.BACKBONE,\n",
    "        num_tiles=num_tiles,\n",
    "        mlp_hidden_dims=config.MLP_HIDDEN_DIMS,\n",
    "        dropout=config.DROPOUT,\n",
    "        offline_backbone_path=config.OFFLINE_BACKBONE_PATH if config.USE_OFFLINE_BACKBONE else None\n",
    "    )\n",
    "    \n",
    "    print(f\"Loading trained checkpoint: {checkpoint_path}\")\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    \n",
    "    # 1. Extract the state dict\n",
    "    state_dict = ckpt['model_state_dict'] if 'model_state_dict' in ckpt else ckpt\n",
    "    \n",
    "    # 2. Clean up keys (remove 'module.' from DataParallel if it exists)\n",
    "    new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    \n",
    "    # 3. CRITICAL: Use strict=False here too\n",
    "    # This allows the model to ignore 'backbone.mask_token' since timm doesn't use it\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)\n",
    "    \n",
    "    if len(unexpected_keys) > 0:\n",
    "        print(f\"Note: Ignored unexpected keys: {unexpected_keys}\")\n",
    "    \n",
    "    model.to(device).eval()\n",
    "    print(\"✓ Trained model loaded successfully!\")\n",
    "    return model\n",
    "\n",
    "def apply_constraint(preds, blend=0.3):\n",
    "    # green, dead, clover, total\n",
    "    component_sum = preds[:, 0] + preds[:, 1] + preds[:, 2]\n",
    "    preds[:, 4] = (1 - blend) * preds[:, 4] + blend * component_sum\n",
    "    return np.maximum(preds, 0)\n",
    "\n",
    "def main():\n",
    "    config = InferenceConfig()\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    # Path detection\n",
    "    if config.DATA_DIR is None:\n",
    "        for d, _, f in os.walk('/kaggle/input'):\n",
    "            if 'test.csv' in f:\n",
    "                config.DATA_DIR = d\n",
    "                break\n",
    "    \n",
    "    test_df_raw = pd.read_csv(Path(config.DATA_DIR) / 'test.csv')\n",
    "    test_df_raw['image_id'] = test_df_raw['sample_id'].str.split('__').str[0]\n",
    "    unique_images = test_df_raw.groupby('image_id').first().reset_index()[['image_id', 'image_path']]\n",
    "    \n",
    "    dataset = PastureTestDataset(unique_images, config.DATA_DIR, TileTransform(config.TILE_SIZE, config.USE_GLOBAL_STREAM))\n",
    "    loader = DataLoader(dataset, batch_size=config.BATCH_SIZE, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
    "    \n",
    "    # Placeholder for accumulated ensemble predictions\n",
    "    ensemble_preds = None\n",
    "    all_ids = []\n",
    "\n",
    "    # Iterate through each fold\n",
    "    for fold_idx, ckpt_path in enumerate(config.CHECKPOINTS):\n",
    "        print(f\"\\n--- Processing Fold {fold_idx} ---\")\n",
    "        model = load_model(ckpt_path, config, device)\n",
    "        \n",
    "        fold_preds = []\n",
    "        current_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for tiles, img_ids in tqdm(loader, desc=f\"Fold {fold_idx} Inference\"):\n",
    "                out = model(tiles.to(device))\n",
    "                fold_preds.append(out.cpu().numpy())\n",
    "                if fold_idx == 0:  # Only need to collect IDs once\n",
    "                    current_ids.extend(img_ids)\n",
    "        \n",
    "        fold_preds = np.vstack(fold_preds)\n",
    "        \n",
    "        if ensemble_preds is None:\n",
    "            ensemble_preds = fold_preds\n",
    "            all_ids = current_ids\n",
    "        else:\n",
    "            ensemble_preds += fold_preds\n",
    "            \n",
    "        # Clean up memory for the next model\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # 1. Average the log-scale predictions\n",
    "    avg_preds = ensemble_preds / len(config.CHECKPOINTS)\n",
    "\n",
    "    # 2. REVERSE LOG TRANSFORMATION\n",
    "    # Using expm1 because we previously identified log1p usage\n",
    "    avg_preds = np.exp(avg_preds) \n",
    "\n",
    "    # 3. Apply constraints (operating on real gram values)\n",
    "    if config.APPLY_CONSTRAINT: \n",
    "        avg_preds = apply_constraint(avg_preds, config.CONSTRAINT_BLEND)\n",
    "    \n",
    "    # Build Submission\n",
    "    target_names = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "    sub_rows = []\n",
    "    for i, img_id in enumerate(all_ids):\n",
    "        for j, t_name in enumerate(target_names):\n",
    "            sub_rows.append({'sample_id': f\"{img_id}__{t_name}\", 'target': avg_preds[i, j]})\n",
    "    \n",
    "    pd.DataFrame(sub_rows).to_csv(config.OUTPUT_PATH, index=False)\n",
    "    print(f\"\\nEnsemble Complete! Submission saved to {config.OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "sourceId": 287736779,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 287922166,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3327,
     "sourceId": 4535,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 543644,
     "modelInstanceId": 529647,
     "sourceId": 698247,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 544020,
     "modelInstanceId": 530063,
     "sourceId": 698751,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 116.075833,
   "end_time": "2025-12-30T16:14:16.555024",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-30T16:12:20.479191",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
