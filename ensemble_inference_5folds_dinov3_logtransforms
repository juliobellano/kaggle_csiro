{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"},{"sourceId":287736779,"sourceType":"kernelVersion"},{"sourceId":287922166,"sourceType":"kernelVersion"},{"sourceId":4535,"sourceType":"modelInstanceVersion","modelInstanceId":3327,"modelId":986},{"sourceId":698247,"sourceType":"modelInstanceVersion","modelInstanceId":529647,"modelId":543644},{"sourceId":698751,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":530063,"modelId":544020}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input/csiro-biomass/test'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-22T07:21:35.758265Z","iopub.execute_input":"2025-12-22T07:21:35.758973Z","iopub.status.idle":"2025-12-22T07:21:35.763827Z","shell.execute_reply.started":"2025-12-22T07:21:35.758941Z","shell.execute_reply":"2025-12-22T07:21:35.763133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# \"\"\"\n# Create a placeholder submission.csv for CSIRO Biomass Competition\n# \"\"\"\n\n# import pandas as pd\n\n# # Competition data directory\n# DATA_DIR = '/kaggle/input/csiro-biomass'\n\n# # Load test.csv to get the correct sample_ids\n# test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n\n# # Create submission with placeholder values (zeros)\n# submission = pd.DataFrame({\n#     'sample_id': test_df['sample_id'],\n#     'target': 0.0  # Placeholder value\n# })\n\n# # Save submission\n# submission.to_csv('submission.csv', index=False)\n\n# print(\"✅ Created submission.csv with placeholder values\")\n# print(f\"   Shape: {submission.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torchvision\n!pip install torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T07:16:16.519170Z","iopub.execute_input":"2025-12-22T07:16:16.519715Z","iopub.status.idle":"2025-12-22T07:16:24.733473Z","shell.execute_reply.started":"2025-12-22T07:16:16.519685Z","shell.execute_reply":"2025-12-22T07:16:24.732732Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport timm\nimport torch\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nfrom pathlib import Path\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\n# =============================================================================\n# CONFIGURATION\n# =============================================================================\n\nclass InferenceConfig:\n    \"\"\"Inference configuration.\"\"\"\n    # Paths\n    DATA_DIR = None  # Auto-detected\n    CHECKPOINTS = [\n        '/kaggle/input/dinov2-logtransform/pytorch/default/1/best_model.pth',\n        '/kaggle/input/dinov2-logtransform/pytorch/default/1/best_model_1.pth',\n        '/kaggle/input/dinov2-logtransform/pytorch/default/1/best_model_2.pth',\n        '/kaggle/input/dinov2-logtransform/pytorch/default/1/best_model_3.pth',\n        '/kaggle/input/dinov2-logtransform/pytorch/default/1/best_model_4.pth',\n    ]\n    OUTPUT_PATH = '/kaggle/working/submission.csv'\n    \n    # Backbone settings\n    USE_OFFLINE_BACKBONE = True\n    OFFLINE_BACKBONE_PATH = '/kaggle/input/dinov2-backbone/dinov2_vitl14.pth'\n    # Use timm name: 'vit_large_patch14_dinov2' = dinov2_vitl14\n    BACKBONE = 'vit_large_patch14_dinov2' \n    \n    # Model settings\n    USE_GLOBAL_STREAM = True\n    TILE_SIZE = 518\n    MLP_HIDDEN_DIMS = [1024, 512]\n    DROPOUT = 0.3\n    \n    # Inference settings\n    BATCH_SIZE = 8\n    NUM_WORKERS = 4\n    USE_TTA = False\n    APPLY_CONSTRAINT = True\n    CONSTRAINT_BLEND = 0.3\n\n# =============================================================================\n# DATA TRANSFORMS & DATASET\n# =============================================================================\n\nclass Config:\n    pass\n\nclass TileTransform:\n    def __init__(self, tile_size=518, include_global=True):\n        self.tile_size = tile_size\n        self.include_global = include_global\n        self.normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        self.resize = T.Resize((tile_size, tile_size), interpolation=T.InterpolationMode.BICUBIC, antialias=True)\n    \n    def _crop_tile(self, image, tile_idx):\n        w, h = image.size\n        hw, hh = w // 2, h // 2\n        pos = {0: (0, 0, hw, hh), 1: (hw, 0, w, hh), 2: (0, hh, hw, h), 3: (hw, hh, w, h)}\n        return image.crop(pos[tile_idx])\n    \n    def __call__(self, image):\n        tiles = []\n        for i in range(4):\n            tile = self.resize(self._crop_tile(image, i))\n            tiles.append(self.normalize(T.ToTensor()(tile)))\n        if self.include_global:\n            tiles.append(self.normalize(T.ToTensor()(self.resize(image))))\n        return torch.stack(tiles, dim=0)\n\nclass PastureTestDataset(Dataset):\n    def __init__(self, df, data_dir, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.data_dir = Path(data_dir)\n        self.transform = transform\n    \n    def __len__(self): return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = self.data_dir / row['image_path']\n        image = Image.open(img_path).convert('RGB')\n        tiles = self.transform(image) if self.transform else torch.zeros(5, 3, 518, 518)\n        return tiles, row['image_id']\n\n# =============================================================================\n# MODEL ARCHITECTURE (Fixed for timm)\n# =============================================================================\n\nclass TiledDinoModel(nn.Module):\n    def __init__(self, backbone_name, num_tiles=5, mlp_hidden_dims=[1024, 512], \n                 dropout=0.3, freeze_backbone=True, offline_backbone_path=None):\n        super().__init__()\n        \n        print(f\"Building backbone: {backbone_name}\")\n        self.backbone = timm.create_model(backbone_name, pretrained=False, num_classes=0)\n        \n        if offline_backbone_path and Path(offline_backbone_path).exists():\n            print(f\"Loading weights from: {offline_backbone_path}\")\n            sd = torch.load(offline_backbone_path, map_location='cuda', weights_only=True)\n            self.backbone.load_state_dict(sd, strict=False)\n        \n        self.feature_dim = self.backbone.num_features\n        if freeze_backbone:\n            for p in self.backbone.parameters(): p.requires_grad = False\n            \n        layers = []\n        prev_dim = num_tiles * self.feature_dim\n        for h in mlp_hidden_dims:\n            layers.extend([nn.Linear(prev_dim, h), nn.ReLU(inplace=True), nn.Dropout(dropout)])\n            prev_dim = h\n        layers.append(nn.Linear(prev_dim, 5))\n        self.head = nn.Sequential(*layers)\n\n    def forward(self, tiles):\n        # tiles shape: (B, N, C, H, W)\n        B, N, C, H, W = tiles.shape\n        \n        # 1. Flatten for backbone\n        flat = tiles.view(B * N, C, H, W)\n        \n        # 2. Extract features\n        features = self.backbone(flat) # Expected shape: (B*N, Dim)\n        \n        # 3. FIX: Ensure tensor is contiguous before changing shape\n        # We use .reshape() which is safer than .view() here\n        features = features.reshape(B, N * self.feature_dim) \n        \n        # 4. Pass through MLP head\n        return self.head(features)\n\n# =============================================================================\n# CORE FUNCTIONS\n# =============================================================================\n\ndef load_model(checkpoint_path, config, device):\n    num_tiles = 5 if config.USE_GLOBAL_STREAM else 4\n    model = TiledDinoModel(\n        backbone_name=config.BACKBONE,\n        num_tiles=num_tiles,\n        mlp_hidden_dims=config.MLP_HIDDEN_DIMS,\n        dropout=config.DROPOUT,\n        offline_backbone_path=config.OFFLINE_BACKBONE_PATH if config.USE_OFFLINE_BACKBONE else None\n    )\n    \n    print(f\"Loading trained checkpoint: {checkpoint_path}\")\n    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n    \n    # 1. Extract the state dict\n    state_dict = ckpt['model_state_dict'] if 'model_state_dict' in ckpt else ckpt\n    \n    # 2. Clean up keys (remove 'module.' from DataParallel if it exists)\n    new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n    \n    # 3. CRITICAL: Use strict=False here too\n    # This allows the model to ignore 'backbone.mask_token' since timm doesn't use it\n    missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)\n    \n    if len(unexpected_keys) > 0:\n        print(f\"Note: Ignored unexpected keys: {unexpected_keys}\")\n    \n    model.to(device).eval()\n    print(\"✓ Trained model loaded successfully!\")\n    return model\n\ndef apply_constraint(preds, blend=0.3):\n    # green, dead, clover, total\n    component_sum = preds[:, 0] + preds[:, 1] + preds[:, 2]\n    preds[:, 4] = (1 - blend) * preds[:, 4] + blend * component_sum\n    return np.maximum(preds, 0)\n\ndef main():\n    config = InferenceConfig()\n    device = torch.device('cuda')\n    \n    # Path detection\n    if config.DATA_DIR is None:\n        for d, _, f in os.walk('/kaggle/input'):\n            if 'test.csv' in f:\n                config.DATA_DIR = d\n                break\n    \n    test_df_raw = pd.read_csv(Path(config.DATA_DIR) / 'test.csv')\n    test_df_raw['image_id'] = test_df_raw['sample_id'].str.split('__').str[0]\n    unique_images = test_df_raw.groupby('image_id').first().reset_index()[['image_id', 'image_path']]\n    \n    dataset = PastureTestDataset(unique_images, config.DATA_DIR, TileTransform(config.TILE_SIZE, config.USE_GLOBAL_STREAM))\n    loader = DataLoader(dataset, batch_size=config.BATCH_SIZE, num_workers=config.NUM_WORKERS, pin_memory=True)\n    \n    # Placeholder for accumulated ensemble predictions\n    ensemble_preds = None\n    all_ids = []\n\n    # Iterate through each fold\n    for fold_idx, ckpt_path in enumerate(config.CHECKPOINTS):\n        print(f\"\\n--- Processing Fold {fold_idx} ---\")\n        model = load_model(ckpt_path, config, device)\n        \n        fold_preds = []\n        current_ids = []\n        \n        with torch.no_grad():\n            for tiles, img_ids in tqdm(loader, desc=f\"Fold {fold_idx} Inference\"):\n                out = model(tiles.to(device))\n                fold_preds.append(out.cpu().numpy())\n                if fold_idx == 0:  # Only need to collect IDs once\n                    current_ids.extend(img_ids)\n        \n        fold_preds = np.vstack(fold_preds)\n        \n        if ensemble_preds is None:\n            ensemble_preds = fold_preds\n            all_ids = current_ids\n        else:\n            ensemble_preds += fold_preds\n            \n        # Clean up memory for the next model\n        del model\n        torch.cuda.empty_cache()\n\n    # 1. Average the log-scale predictions\n    avg_preds = ensemble_preds / len(config.CHECKPOINTS)\n\n    # 2. REVERSE LOG TRANSFORMATION\n    # Using expm1 because we previously identified log1p usage\n    avg_preds = np.exp(avg_preds) \n\n    # 3. Apply constraints (operating on real gram values)\n    if config.APPLY_CONSTRAINT: \n        avg_preds = apply_constraint(avg_preds, config.CONSTRAINT_BLEND)\n    \n    # Build Submission\n    target_names = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n    sub_rows = []\n    for i, img_id in enumerate(all_ids):\n        for j, t_name in enumerate(target_names):\n            sub_rows.append({'sample_id': f\"{img_id}__{t_name}\", 'target': avg_preds[i, j]})\n    \n    pd.DataFrame(sub_rows).to_csv(config.OUTPUT_PATH, index=False)\n    print(f\"\\nEnsemble Complete! Submission saved to {config.OUTPUT_PATH}\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T01:27:03.227850Z","iopub.execute_input":"2025-12-26T01:27:03.228091Z","iopub.status.idle":"2025-12-26T01:32:53.409169Z","shell.execute_reply.started":"2025-12-26T01:27:03.228067Z","shell.execute_reply":"2025-12-26T01:32:53.408170Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n--- Processing Fold 0 ---\nBuilding backbone: vit_large_patch14_dinov2\nLoading weights from: /kaggle/input/dinov2-backbone/dinov2_vitl14.pth\nLoading trained checkpoint: /kaggle/input/dinov2-logtransform/pytorch/default/1/best_model.pth\nNote: Ignored unexpected keys: ['tile_pos_embed', 'backbone.mask_token']\n✓ Trained model loaded successfully!\n","output_type":"stream"},{"name":"stderr","text":"Fold 0 Inference:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\nFold 0 Inference: 100%|██████████| 1/1 [00:49<00:00, 49.23s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Processing Fold 1 ---\nBuilding backbone: vit_large_patch14_dinov2\nLoading weights from: /kaggle/input/dinov2-backbone/dinov2_vitl14.pth\nLoading trained checkpoint: /kaggle/input/dinov2-logtransform/pytorch/default/1/best_model_1.pth\nNote: Ignored unexpected keys: ['tile_pos_embed', 'backbone.mask_token']\n✓ Trained model loaded successfully!\n","output_type":"stream"},{"name":"stderr","text":"Fold 1 Inference: 100%|██████████| 1/1 [00:46<00:00, 46.38s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Processing Fold 2 ---\nBuilding backbone: vit_large_patch14_dinov2\nLoading weights from: /kaggle/input/dinov2-backbone/dinov2_vitl14.pth\nLoading trained checkpoint: /kaggle/input/dinov2-logtransform/pytorch/default/1/best_model_2.pth\nNote: Ignored unexpected keys: ['tile_pos_embed', 'backbone.mask_token']\n✓ Trained model loaded successfully!\n","output_type":"stream"},{"name":"stderr","text":"Fold 2 Inference: 100%|██████████| 1/1 [00:46<00:00, 46.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Processing Fold 3 ---\nBuilding backbone: vit_large_patch14_dinov2\nLoading weights from: /kaggle/input/dinov2-backbone/dinov2_vitl14.pth\nLoading trained checkpoint: /kaggle/input/dinov2-logtransform/pytorch/default/1/best_model_3.pth\nNote: Ignored unexpected keys: ['tile_pos_embed', 'backbone.mask_token']\n✓ Trained model loaded successfully!\n","output_type":"stream"},{"name":"stderr","text":"Fold 3 Inference: 100%|██████████| 1/1 [00:46<00:00, 46.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Processing Fold 4 ---\nBuilding backbone: vit_large_patch14_dinov2\nLoading weights from: /kaggle/input/dinov2-backbone/dinov2_vitl14.pth\nLoading trained checkpoint: /kaggle/input/dinov2-logtransform/pytorch/default/1/best_model_4.pth\nNote: Ignored unexpected keys: ['tile_pos_embed', 'backbone.mask_token']\n✓ Trained model loaded successfully!\n","output_type":"stream"},{"name":"stderr","text":"Fold 4 Inference: 100%|██████████| 1/1 [00:46<00:00, 46.24s/it]","output_type":"stream"},{"name":"stdout","text":"\nEnsemble Complete! Submission saved to /kaggle/working/submission.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1}]}